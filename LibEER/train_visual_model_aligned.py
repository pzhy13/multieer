import os
import glob
import pickle
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
from tqdm import tqdm
import argparse

# å¼•å…¥ LibEER å·¥å…·
from config.setting import preset_setting, set_setting_by_args
from data_utils.split import get_split_index
from utils.args import get_args_parser
from utils.utils import setup_seed
from utils.store import make_output_dir

# ğŸ”´ HSEmotion è¡¥ä¸
_original_load = torch.load
def safe_load_patch(*args, **kwargs):
    if 'weights_only' not in kwargs: kwargs['weights_only'] = False
    return _original_load(*args, **kwargs)
torch.load = safe_load_patch
from hsemotion.facial_emotions import HSEmotionRecognizer

# ================= é…ç½®åŒº =================
MINI_BATCH_SIZE = 8
TARGET_BATCH_SIZE = 32
ACCUMULATION_STEPS = TARGET_BATCH_SIZE // MINI_BATCH_SIZE

# ================= è¾…åŠ©ç±» =================
class VisualDataset(Dataset):
    def __init__(self, data_list, label_list, transform=None):
        self.data_list = data_list
        self.label_list = label_list
        self.transform = transform

    def __len__(self):
        return len(self.data_list)

    def __getitem__(self, idx):
        img_path = self.data_list[idx]
        label_val = self.label_list[idx] # è¿™é‡Œ label_list å·²ç»æ˜¯å±•å¹³åçš„ int åˆ—è¡¨
        try:
            img = Image.open(img_path).convert('RGB')
            if self.transform: img = self.transform(img)
        except:
            print(f"Warning: Failed to load {img_path}")
            img = torch.zeros(3, 224, 224)
        return img, torch.tensor(label_val, dtype=torch.long)

def get_visual_data_aligned(args):
    """
    è¯»å–æ•°æ®å¹¶ç»„ç»‡æˆ List[Subject] -> List[Trail] -> List[Samples] çš„ç»“æ„
    ä»¥ä¾¿ LibEER çš„ split.py å¯ä»¥æŒ‰ç…§ Trail è¿›è¡Œåˆ’åˆ† (Cross-Trail)
    """
    print(f"æ­£åœ¨æ„å»ºè§†è§‰æ•°æ®é›†ç´¢å¼• (Cross-Trail Mode)...")
    all_labels = {}
    # è¯»å–æ‰€æœ‰è¢«è¯•çš„æ ‡ç­¾æ–‡ä»¶
    for sub_id in range(1, 33):
        path = os.path.join(args.dataset_path, f"s{sub_id:02d}.dat")
        if os.path.exists(path):
            with open(path, 'rb') as f:
                content = pickle.load(f, encoding='latin1')
                all_labels[sub_id] = content['labels']
        else:
            print(f"Warning: Label file for s{sub_id:02d} not found.")

    aligned_data = []  # ç»“æ„: [Subject_1_Trails, Subject_2_Trails, ...]
    aligned_label = [] # ç»“æ„: [Subject_1_Labels, Subject_2_Labels, ...]

    for sub_id in range(1, 33):
        sub_str = f"s{sub_id:02d}"
        sub_trails_data = []  # å­˜æ”¾è¯¥è¢«è¯•æ‰€æœ‰ Trail çš„å›¾ç‰‡è·¯å¾„åˆ—è¡¨
        sub_trails_label = [] # å­˜æ”¾è¯¥è¢«è¯•æ‰€æœ‰ Trail çš„æ ‡ç­¾åˆ—è¡¨
        
        sub_face_dir = os.path.join(args.faces_path, sub_str)
        
        if not os.path.exists(sub_face_dir) or sub_id not in all_labels:
            aligned_data.append([])
            aligned_label.append([])
            continue

        # DEAP æ•°æ®é›†é€šå¸¸æœ‰ 40 ä¸ª Trail
        for trial_id in range(1, 41):
            # è·å–è¯¥ Trail çš„æ ‡ç­¾
            raw_label = all_labels[sub_id][trial_id - 1]
            valence, arousal = raw_label[0], raw_label[1]
            v_high = valence >= 5
            a_high = arousal >= 5
            
            # 4åˆ†ç±» (HALV) é€»è¾‘
            if not v_high and not a_high: cls = 0   # LALV
            elif not v_high and a_high:   cls = 1   # LAHV
            elif v_high and not a_high:   cls = 2   # HALV
            elif v_high and a_high:       cls = 3   # HAHV
            
            # è·å–è¯¥ Trail ä¸‹çš„æ‰€æœ‰ Segment å›¾ç‰‡
            pattern = f"{sub_str}_trial{trial_id:02d}_seg*.jpg"
            search_path = os.path.join(sub_face_dir, pattern)
            files = glob.glob(search_path)
            # æŒ‰ segment ID æ’åºç¡®ä¿é¡ºåº
            files.sort(key=lambda x: int(x.split('_seg')[-1].split('.')[0]))
            
            if len(files) > 0:
                # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬å°†ä¸€ä¸ª Trail çš„æ‰€æœ‰å›¾ç‰‡ä½œä¸ºä¸€ä¸ªæ•´ä½“å­˜å…¥åˆ—è¡¨
                # split.py ä¼šæ ¹æ®è¿™ä¸ªåˆ—è¡¨çš„é•¿åº¦ï¼ˆå³ Trail çš„æ•°é‡ï¼‰è¿›è¡Œåˆ’åˆ†
                sub_trails_data.append(files)
                # æ ‡ç­¾ä¹Ÿè¦å¯¹åº”å›¾ç‰‡çš„æ•°é‡ï¼Œé‡å¤ cls
                sub_trails_label.append([cls] * len(files))

        aligned_data.append(sub_trails_data)
        aligned_label.append(sub_trails_label)
        print(f"Subject {sub_str}: Loaded {len(sub_trails_data)} trails.")
        
    return aligned_data, aligned_label

def flatten_data(data_trails, label_trails, indices):
    """
    è¾…åŠ©å‡½æ•°ï¼šå°†é€‰ä¸­çš„ Trail ç´¢å¼•å¯¹åº”çš„å›¾ç‰‡å’Œæ ‡ç­¾å±•å¹³æˆä¸€ç»´åˆ—è¡¨
    """
    flat_data = []
    flat_label = []
    for i in indices:
        flat_data.extend(data_trails[i])
        flat_label.extend(label_trails[i])
    return flat_data, flat_label

# ================= ä¸»ç¨‹åº =================
def main(args):
    # å¼ºåˆ¶è®¾ç½® split ç›¸å…³çš„å‚æ•°ä»¥ç¬¦åˆä½ çš„è¦æ±‚
    if args.setting is None:
        # å¦‚æœæ²¡æœ‰æŒ‡å®š presetï¼Œæ‰‹åŠ¨æ„å»ºä¸€ä¸ªåŸºç¡€ setting
        setting = set_setting_by_args(args)
    else:
        setting = preset_setting[args.setting](args)

    # ç¡®ä¿å®éªŒæ¨¡å¼æ˜¯ subject-dependent
    # æ³¨æ„ï¼šsplit.py ä¾èµ– setting å¯¹è±¡é‡Œçš„å±æ€§
    # å¦‚æœ args é‡Œæ²¡æœ‰ä¼ è¿™äº›å‚æ•°ï¼Œè¿™é‡Œæœ€å¥½å¼ºåˆ¶è¦†ç›–ä¸€ä¸‹ï¼Œæˆ–è€…ç¡®ä¿å‘½ä»¤è¡Œä¼ å…¥äº†æ­£ç¡®çš„å‚æ•°
    # ä¾‹å¦‚: --experiment_mode subject-dependent --split_type train-val-test
    
    setup_seed(args.seed)
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ Training on: {device} (Gradient Accumulation Mode)")

    # 1. è·å–æŒ‰ Trail ç»„ç»‡çš„æ•°æ®
    data_all_subs, label_all_subs = get_visual_data_aligned(args)
    
    train_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(),
        transforms.ColorJitter(brightness=0.1, contrast=0.1),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    val_test_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    test_accuracies = [] # è®°å½•æ‰€æœ‰è¢«è¯•åœ¨ Test é›†ä¸Šçš„æœ€ç»ˆå‡†ç¡®ç‡

    # 2. éå†æ¯ä¸ª Subject (Subject-Dependent)
    for rridx, (data_trails, label_trails) in enumerate(zip(data_all_subs, label_all_subs), 1):
        if len(data_trails) == 0:
            continue
        
        # ä½¿ç”¨ LibEER çš„åˆ’åˆ†é€»è¾‘
        # get_split_index ä¼šè¿”å› Trail çš„ç´¢å¼• (å› ä¸ºä¼ å…¥çš„ data_trails æ˜¯ List[Trail])
        tts = get_split_index(data_trails, label_trails, setting)
        
        print(f"\n========== Subject {rridx} Training ==========")

        # éå†åˆ’åˆ† (é€šå¸¸ train-val-test åªæœ‰ 1 ä¸ª foldï¼Œk-fold ä¼šæœ‰å¤šä¸ª)
        for ridx, (train_indexes, test_indexes, val_indexes) in enumerate(zip(tts['train'], tts['test'], tts['val']), 1):
            setup_seed(args.seed)
            
            # å¤„ç†éªŒè¯é›†ä¸ºç©ºçš„æƒ…å†µ (LibEER é€»è¾‘)
            if val_indexes[0] == -1 or len(val_indexes) == 0:
                print("Notice: No validation set provided, using test set for validation.")
                val_indexes = test_indexes

            print(f"Fold {ridx} - Train Trails: {len(train_indexes)}, Val Trails: {len(val_indexes)}, Test Trails: {len(test_indexes)}")

            # 3. å°† Trail ç´¢å¼•å±•å¹³ä¸ºå›¾ç‰‡æ ·æœ¬
            train_paths, train_lbls = flatten_data(data_trails, label_trails, train_indexes)
            val_paths, val_lbls = flatten_data(data_trails, label_trails, val_indexes)
            test_paths, test_lbls = flatten_data(data_trails, label_trails, test_indexes)

            # æ„å»º Dataset å’Œ DataLoader
            train_set = VisualDataset(train_paths, train_lbls, transform=train_transform)
            val_set = VisualDataset(val_paths, val_lbls, transform=val_test_transform)
            test_set = VisualDataset(test_paths, test_lbls, transform=val_test_transform)

            train_loader = DataLoader(train_set, batch_size=MINI_BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=False)
            val_loader = DataLoader(val_set, batch_size=MINI_BATCH_SIZE * 2, shuffle=False, num_workers=0, pin_memory=False)
            test_loader = DataLoader(test_set, batch_size=MINI_BATCH_SIZE * 2, shuffle=False, num_workers=0, pin_memory=False)

            # åˆå§‹åŒ–æ¨¡å‹
            fer = HSEmotionRecognizer(model_name='enet_b0_8_va_mtl', device='cpu')
            model = fer.model
            
            # ä¿®æ”¹åˆ†ç±»å¤´é€‚é… 4 åˆ†ç±»
            num_ftrs = 1280
            try:
                if hasattr(model, 'num_features'): num_ftrs = model.num_features
                elif hasattr(model, 'classifier') and not isinstance(model.classifier, nn.Identity): num_ftrs = model.classifier.in_features
                elif hasattr(model, 'fc') and not isinstance(model.fc, nn.Identity): num_ftrs = model.fc.in_features
            except: pass
            
            model.classifier = nn.Linear(num_ftrs, 4)
            if hasattr(model, 'fc'): model.fc = nn.Identity()
            model.to(device)

            optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-4)
            criterion = nn.CrossEntropyLoss()

            best_val_acc = 0.0
            best_model_path = os.path.join(args.output_dir, f"visual_model_sub{rridx}_fold{ridx}_best.pth")

            # ğŸŸ¢ğŸŸ¢ğŸŸ¢ è®­ç»ƒå¾ªç¯ ğŸŸ¢ğŸŸ¢ğŸŸ¢
            for epoch in range(args.epochs):
                model.train()
                train_loss = 0
                
                # è®­ç»ƒæ­¥éª¤
                pbar = tqdm(train_loader, desc=f"Train S{rridx} F{ridx} Ep{epoch+1}", leave=False)
                for i, (imgs, targets) in enumerate(pbar):
                    imgs, targets = imgs.to(device), targets.to(device)
                    outputs = model(imgs)
                    loss = criterion(outputs, targets)
                    
                    loss = loss / ACCUMULATION_STEPS
                    loss.backward()
                    
                    if (i + 1) % ACCUMULATION_STEPS == 0:
                        optimizer.step()
                        optimizer.zero_grad()
                    
                    train_loss += loss.item() * ACCUMULATION_STEPS

                # éªŒè¯æ­¥éª¤ (ç”¨äºæ¨¡å‹é€‰æ‹©)
                model.eval()
                val_correct = 0
                val_total = 0
                with torch.no_grad():
                    for imgs, targets in val_loader:
                        imgs, targets = imgs.to(device), targets.to(device)
                        outputs = model(imgs)
                        _, preds = torch.max(outputs, 1)
                        val_correct += torch.sum(preds == targets.data)
                        val_total += targets.size(0)
                
                val_acc = val_correct.double() / val_total if val_total > 0 else 0
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if val_acc > best_val_acc:
                    best_val_acc = val_acc
                    torch.save(model.state_dict(), best_model_path)
                
                # print(f"Epoch {epoch+1} Loss: {train_loss/len(train_loader):.4f} Val Acc: {val_acc:.4f}")

            # ğŸŸ¢ğŸŸ¢ğŸŸ¢ æµ‹è¯•æ­¥éª¤ (Test Evaluation) ğŸŸ¢ğŸŸ¢ğŸŸ¢
            # åŠ è½½éªŒè¯é›†ä¸Šè¡¨ç°æœ€å¥½çš„æ¨¡å‹
            if os.path.exists(best_model_path):
                model.load_state_dict(torch.load(best_model_path, map_location=device))
                print(f"Loaded best model with Val Acc: {best_val_acc:.4f}")
            else:
                print("Warning: No model saved, using last epoch model.")

            model.eval()
            test_correct = 0
            test_total = 0
            with torch.no_grad():
                for imgs, targets in tqdm(test_loader, desc=f"Testing S{rridx} F{ridx}"):
                    imgs, targets = imgs.to(device), targets.to(device)
                    outputs = model(imgs)
                    _, preds = torch.max(outputs, 1)
                    test_correct += torch.sum(preds == targets.data)
                    test_total += targets.size(0)
            
            test_acc = test_correct.double() / test_total if test_total > 0 else 0
            print(f"ğŸ‘‰ Subject {rridx} Fold {ridx} TEST ACCURACY: {test_acc:.4f}")
            test_accuracies.append(test_acc.item())

    print("\n========== Final Results ==========")
    print(f"Average Test Accuracy across {len(test_accuracies)} folds/subjects: {np.mean(test_accuracies):.4f}")

if __name__ == '__main__':
    parser = get_args_parser()
    parser.add_argument('-faces_path', type=str, required=True, help='Path to the face images directory')
    # å»ºè®®è¿è¡Œæ—¶æ·»åŠ ä»¥ä¸‹å‚æ•°ä»¥ç¡®ä¿ split é€»è¾‘æ­£ç¡®:
    # -experiment_mode subject-dependent -split_type train-val-test -test_size 0.2 -val_size 0.1
    args = parser.parse_args()
    
    args.output_dir = make_output_dir(args, "VisualModel")
    if not os.path.exists(args.output_dir):
        os.makedirs(args.output_dir)
    print(f"ğŸ“‚ Output directory: {args.output_dir}")
    
    main(args)